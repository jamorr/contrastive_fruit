{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = r\"..\\data\\Lemon\\annotations\\instances_default.json\"\n",
    "image_path = \"..\\data\\Lemon\\images\"\n",
    "\n",
    "with open(annotation_path) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "labels = [(e[\"image_id\"], e[\"category_id\"]) for e in annotations[\"annotations\"]]\n",
    "category_names = [(e[\"id\"], e[\"name\"]) for e in annotations[\"categories\"]]\n",
    "labels_df = pd.DataFrame(labels, columns=[\"image_id\", \"Category id\"])\n",
    "# labels_df.set_index(\"id\", inplace=True)\n",
    "category_names_df = pd.DataFrame(category_names, columns=[\"Category id\", \"Label\"])\n",
    "category_names_df.set_index(\"Category id\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pedicel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>illness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6502</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6502</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6502</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6502</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6502</th>\n",
       "      <td>blemish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33753 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label\n",
       "image_id         \n",
       "0         pedicel\n",
       "0         blemish\n",
       "0         blemish\n",
       "100       blemish\n",
       "100       illness\n",
       "...           ...\n",
       "6502      blemish\n",
       "6502      blemish\n",
       "6502      blemish\n",
       "6502      blemish\n",
       "6502      blemish\n",
       "\n",
       "[33753 rows x 1 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_named_df = pd.merge(labels_df, category_names_df, \"left\", \"Category id\").drop(\"Category id\", axis=1)\n",
    "labels_named_df.set_index(\"image_id\", inplace=True)\n",
    "labels_named_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Label\n",
       "image_id           \n",
       "2166      condition\n",
       "3400      condition"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_named_df[labels_named_df[\"Label\"]==\"condition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.embedding import SelfSupervisedEmbedding\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models import ResNetGenerator\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms import SimCLRTransform, utils\n",
    "# Set the path to your dataset\n",
    "# data_path = '/path/to/your/dataset'\n",
    "\n",
    "\n",
    "# Initialize a ResNet model from lightly\n",
    "resnet = ResNetGenerator('resnet-18', num_classes=len(category_names_df))\n",
    "\n",
    "\n",
    "# Define the loss function (NTXentLoss)\n",
    "criterion = NTXentLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "# optimizer = torch.optim.Adam(embedding_model.parameters(), lr=3e-4)\n",
    "\n",
    "# Initialize the self-supervised embedding model\n",
    "embedding_model = SelfSupervisedEmbedding(resnet, NTXentLoss(), torch.optim.Adam, loader)\n",
    "\n",
    "# Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Morri\\Documents\\Notebooks\\AI\\contrastive\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: 8.05545\n",
      "epoch: 01, loss: 7.50374\n",
      "epoch: 02, loss: 7.48619\n",
      "epoch: 03, loss: 7.56141\n",
      "epoch: 04, loss: 7.59193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\AI\\contrastive\\src\\testing.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/AI/contrastive/src/testing.ipynb#X23sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(teacher_out, student_out, epoch\u001b[39m=\u001b[39mepoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/AI/contrastive/src/testing.ipynb#X23sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/AI/contrastive/src/testing.ipynb#X23sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/AI/contrastive/src/testing.ipynb#X23sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# We only cancel gradients of student head.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/AI/contrastive/src/testing.ipynb#X23sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m model\u001b[39m.\u001b[39mstudent_head\u001b[39m.\u001b[39mcancel_last_layer_gradients(current_epoch\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\AI\\contrastive\\.venv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\AI\\contrastive\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import masked_autoencoder\n",
    "from lightly.transforms.mae_transform import MAETransform  # Same transform as MAE\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "\n",
    "\n",
    "class DINO(torch.nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "\n",
    "resnet = torchvision.models.resnet18()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "input_dim = 512\n",
    "# instead of a resnet you can also use a vision transformer backbone as in the\n",
    "# original paper (you might have to reduce the batch size in this case):\n",
    "# backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=False)\n",
    "# input_dim = backbone.embed_dim\n",
    "\n",
    "model = DINO(backbone, input_dim)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "transform = DINOTransform()\n",
    "dataset = LightlyDataset(image_path,transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "criterion = DINOLoss(\n",
    "    output_dim=2048,\n",
    "    warmup_teacher_temp_epochs=5,\n",
    ")\n",
    "# move loss to correct device because it also contains parameters\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    momentum_val = cosine_schedule(epoch, epochs, 0.996, 1)\n",
    "    for batch in dataloader:\n",
    "        views = batch[0]\n",
    "        update_momentum(model.student_backbone, model.teacher_backbone, m=momentum_val)\n",
    "        update_momentum(model.student_head, model.teacher_head, m=momentum_val)\n",
    "        views = [view.to(device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [model.forward_teacher(view) for view in global_views]\n",
    "        student_out = [model.forward(view) for view in views]\n",
    "        loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        # We only cancel gradients of student head.\n",
    "        model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
